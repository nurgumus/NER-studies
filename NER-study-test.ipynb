{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nimport json\nimport pandas as pd\n\n# URL to the raw JSON file\nurl = 'https://raw.githubusercontent.com/turkish-nlp-suite/Vitamins-Supplements-NER-dataset/main/supplements_spans_ents.json'\n\n# Fetch the raw JSON data\nresponse = requests.get(url)\ndata = response.json()  # Parse the JSON data\n\n# Initialize an empty list to store processed rows\nrows = []\n\n# Iterate over each item in the JSON data\nfor item in data:\n    sent_id = item['sent_id']\n    text = item['text']\n    entities = item['entities']\n    \n    # Iterate over each entity in the entities list\n    for entity in entities:\n        row = {\n            'sent_id': sent_id,\n            'text': text,\n            'entity_start': entity['start'],\n            'entity_end': entity['end'],\n            'entity_label': entity['label'],\n            'entity_value': entity['val']\n        }\n        rows.append(row)\n\n# Create a DataFrame from the list of rows\ndf = pd.DataFrame(rows)\n\n# Display the DataFrame\nprint(df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:19:08.359741Z","iopub.execute_input":"2024-05-17T17:19:08.360778Z","iopub.status.idle":"2024-05-17T17:19:08.495508Z","shell.execute_reply.started":"2024-05-17T17:19:08.360744Z","shell.execute_reply":"2024-05-17T17:19:08.494554Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"       sent_id                                               text  \\\n0        82838  Henüz 4 gündür kullanıyorum ama iyi geldi gibi...   \n1        82838  Henüz 4 gündür kullanıyorum ama iyi geldi gibi...   \n2        82838  Henüz 4 gündür kullanıyorum ama iyi geldi gibi...   \n3        85120  Diğer demir ilaçlarından farklı olarak mideyi ...   \n4        85120  Diğer demir ilaçlarından farklı olarak mideyi ...   \n...        ...                                                ...   \n10095   207704  Bana göre multivitamin almak için yorumlara ba...   \n10096   207704  Bana göre multivitamin almak için yorumlara ba...   \n10097   207704  Bana göre multivitamin almak için yorumlara ba...   \n10098   207704  Bana göre multivitamin almak için yorumlara ba...   \n10099   207704  Bana göre multivitamin almak için yorumlara ba...   \n\n       entity_start  entity_end entity_label  \\\n0               119         160         ETKİ   \n1               198         208          DOZ   \n2               309         328         ETKİ   \n3                 6          24   ÜRÜN_DİĞER   \n4                39          62         ETKİ   \n...             ...         ...          ...   \n10095            94         106  BİYOMOLEKÜL   \n10096           182         209         ETKİ   \n10097           248         269         ETKİ   \n10098           163         178     TAT_KOKU   \n10099           248         261     HASTALIK   \n\n                                    entity_value  \n0      halsizliği enerji düşüklüğünü yaşamıyorum  \n1                                     bir tablet  \n2                            iştahım da açılmadı  \n3                             demir ilaçlarından  \n4                        mideyi rahatsız etmiyor  \n...                                          ...  \n10095                               multivitamin  \n10096                midede rahatsızlık yapmıyor  \n10097                      saç dökülmesi kesildi  \n10098                            daha az kokuyor  \n10099                              saç dökülmesi  \n\n[10100 rows x 6 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ndef is_gpu_available():\n    \"\"\"\n    Check if GPU is available for computation using PyTorch.\n    Returns True if GPU is available, False otherwise.\n    \"\"\"\n    return torch.cuda.is_available()\n\n# Example usage\nif is_gpu_available():\n    print(\"GPU is available.\")\nelse:\n    print(\"No GPU available, using CPU instead.\")\n\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:19:12.594450Z","iopub.execute_input":"2024-05-17T17:19:12.595142Z","iopub.status.idle":"2024-05-17T17:19:15.783532Z","shell.execute_reply.started":"2024-05-17T17:19:12.595100Z","shell.execute_reply":"2024-05-17T17:19:15.782515Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU is available.\n","output_type":"stream"}]},{"cell_type":"code","source":"import datasets \nimport numpy as np \nfrom transformers import BertTokenizerFast \nfrom transformers import DataCollatorForTokenClassification \nfrom transformers import AutoModelForTokenClassification ","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ndataset = Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:19:22.882415Z","iopub.execute_input":"2024-05-17T17:19:22.883234Z","iopub.status.idle":"2024-05-17T17:19:23.891331Z","shell.execute_reply.started":"2024-05-17T17:19:22.883198Z","shell.execute_reply":"2024-05-17T17:19:23.890462Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:19:25.806528Z","iopub.execute_input":"2024-05-17T17:19:25.807507Z","iopub.status.idle":"2024-05-17T17:19:25.813935Z","shell.execute_reply.started":"2024-05-17T17:19:25.807472Z","shell.execute_reply":"2024-05-17T17:19:25.813065Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['sent_id', 'text', 'entity_start', 'entity_end', 'entity_label', 'entity_value'],\n    num_rows: 10100\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_testvalid = dataset.train_test_split(test_size=0.1, seed = 42)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:19:28.002369Z","iopub.execute_input":"2024-05-17T17:19:28.002728Z","iopub.status.idle":"2024-05-17T17:19:28.024113Z","shell.execute_reply.started":"2024-05-17T17:19:28.002699Z","shell.execute_reply":"2024-05-17T17:19:28.023424Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:19:29.566746Z","iopub.execute_input":"2024-05-17T17:19:29.567147Z","iopub.status.idle":"2024-05-17T17:19:29.582142Z","shell.execute_reply.started":"2024-05-17T17:19:29.567109Z","shell.execute_reply":"2024-05-17T17:19:29.581136Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict\ntrain_test_valid_dataset = DatasetDict({\n'train': train_testvalid['train'],\n'test': test_valid['test'],\n'valid': test_valid['train']})\n\ntrain_test_valid_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:19:35.186318Z","iopub.execute_input":"2024-05-17T17:19:35.186904Z","iopub.status.idle":"2024-05-17T17:19:35.193679Z","shell.execute_reply.started":"2024-05-17T17:19:35.186872Z","shell.execute_reply":"2024-05-17T17:19:35.192775Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sent_id', 'text', 'entity_start', 'entity_end', 'entity_label', 'entity_value'],\n        num_rows: 9090\n    })\n    test: Dataset({\n        features: ['sent_id', 'text', 'entity_start', 'entity_end', 'entity_label', 'entity_value'],\n        num_rows: 505\n    })\n    valid: Dataset({\n        features: ['sent_id', 'text', 'entity_start', 'entity_end', 'entity_label', 'entity_value'],\n        num_rows: 505\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_test_valid_dataset['train'].unique('entity_label')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:19:40.726731Z","iopub.execute_input":"2024-05-17T17:19:40.727316Z","iopub.status.idle":"2024-05-17T17:19:41.348878Z","shell.execute_reply.started":"2024-05-17T17:19:40.727282Z","shell.execute_reply":"2024-05-17T17:19:41.348056Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/9090 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50ec336e1e0423daa98643e75bd83db"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['ÜRÜN_DİĞER',\n 'ETKİ',\n 'TAT_KOKU',\n 'HASTALIK',\n 'BİYOMOLEKÜL',\n 'MARKA_DİĞER',\n 'DOZ',\n 'SAĞLIK_ŞİKAYETLERİ',\n 'MARKA',\n 'KULLANICI',\n 'YAN_ETKİ',\n 'İÇERİK',\n 'TAVSİYE_EDEN',\n 'KULLANICI_DEMOGRAFİSİ']"},"metadata":{}}]},{"cell_type":"code","source":"label_list = train_test_valid_dataset['train'].unique('entity_label')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:43:47.725109Z","iopub.execute_input":"2024-05-17T17:43:47.725468Z","iopub.status.idle":"2024-05-17T17:43:48.347594Z","shell.execute_reply.started":"2024-05-17T17:43:47.725439Z","shell.execute_reply":"2024-05-17T17:43:48.346653Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/9090 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8008a8959cb9480893c7568c7d2bd4e5"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained(\"dbmdz/bert-base-turkish-uncased\") ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:20:46.036444Z","iopub.execute_input":"2024-05-17T17:20:46.037250Z","iopub.status.idle":"2024-05-17T17:20:48.906501Z","shell.execute_reply.started":"2024-05-17T17:20:46.037216Z","shell.execute_reply":"2024-05-17T17:20:48.905568Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b24974f4963542658e7b32b47ff4dc6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/263k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071c37179bd3466eb661f17b28408093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8411d71174ca417ba777989886f7a3ff"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_and_align_labels(example):\n    # Define label-to-id mapping based on the provided labels\n    label_to_id = {\n        'ÜRÜN_DİĞER': 0,\n        'ETKİ': 1,\n        'TAT_KOKU': 2,\n        'HASTALIK': 3,\n        'BİYOMOLEKÜL': 4,\n        'MARKA_DİĞER': 5,\n        'DOZ': 6,\n        'SAĞLIK_ŞİKAYETLERİ': 7,\n        'MARKA': 8,\n        'KULLANICI': 9,\n        'YAN_ETKİ': 10,\n        'İÇERİK': 11,\n        'TAVSİYE_EDEN': 12,\n        'KULLANICI_DEMOGRAFİSİ': 13\n    }\n    \n    tokenized_inputs = tokenizer(example['text'], padding='max_length', truncation=True, return_tensors=\"pt\")\n    labels = []\n\n    for i, text in enumerate(example['text']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their corresponding word indices\n        label_ids = [-100] * len(word_ids)  # Initialize all labels as -100 (ignored index)\n\n        for j, word_id in enumerate(word_ids):\n            if word_id is None:  # Skip special tokens\n                continue\n            token_start = tokenized_inputs.char_to_token(i, example['entity_start'][i])\n            token_end = tokenized_inputs.char_to_token(i, example['entity_end'][i] - 1)  # -1 to include the last character\n            if token_start is None or token_end is None:\n                continue\n            for k in range(token_start, token_end + 1):\n                label_ids[k] = label_to_id[example['entity_label'][i]]  # Convert label to integer using the mapping dictionary\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:24:40.992962Z","iopub.execute_input":"2024-05-17T17:24:40.993336Z","iopub.status.idle":"2024-05-17T17:24:41.003152Z","shell.execute_reply.started":"2024-05-17T17:24:40.993306Z","shell.execute_reply":"2024-05-17T17:24:41.002114Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets_train = train_test_valid_dataset['train'].map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:24:44.003016Z","iopub.execute_input":"2024-05-17T17:24:44.003408Z","iopub.status.idle":"2024-05-17T17:24:57.596256Z","shell.execute_reply.started":"2024-05-17T17:24:44.003379Z","shell.execute_reply":"2024-05-17T17:24:57.595376Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9090 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d491dbe90d9409e9a737158b598080b"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets_test = train_test_valid_dataset['test'].map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:24:59.481216Z","iopub.execute_input":"2024-05-17T17:24:59.481586Z","iopub.status.idle":"2024-05-17T17:25:00.265877Z","shell.execute_reply.started":"2024-05-17T17:24:59.481554Z","shell.execute_reply":"2024-05-17T17:25:00.265013Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d871fb6c5417404ca2c7af081e2e9e27"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets_val = train_test_valid_dataset['valid'].map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:25:03.876497Z","iopub.execute_input":"2024-05-17T17:25:03.876884Z","iopub.status.idle":"2024-05-17T17:25:04.667586Z","shell.execute_reply.started":"2024-05-17T17:25:03.876853Z","shell.execute_reply":"2024-05-17T17:25:04.666673Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6896e916e9f14fdf8a785d5ceaa79f6a"}},"metadata":{}}]},{"cell_type":"code","source":"final_dataset = DatasetDict({\n'train': tokenized_datasets_train,\n'test': tokenized_datasets_test,\n'valid': tokenized_datasets_val})","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:25:19.893204Z","iopub.execute_input":"2024-05-17T17:25:19.893850Z","iopub.status.idle":"2024-05-17T17:25:19.898195Z","shell.execute_reply.started":"2024-05-17T17:25:19.893819Z","shell.execute_reply":"2024-05-17T17:25:19.897129Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming tokenized_datasets_train[0] contains tokenized data\nexample = final_dataset['train'][0]\n\n# Extract tokens and convert them to strings\ntokens = tokenizer.convert_ids_to_tokens(example[\"input_ids\"])\n\n# Create a DataFrame with tokens\ndf = pd.DataFrame({'Token': tokens})\n\n# Display the DataFrame\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:33:15.326650Z","iopub.execute_input":"2024-05-17T17:33:15.327005Z","iopub.status.idle":"2024-05-17T17:33:15.337770Z","shell.execute_reply.started":"2024-05-17T17:33:15.326967Z","shell.execute_reply":"2024-05-17T17:33:15.336747Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"     Token\n0    [CLS]\n1        3\n2        .\n3      sis\n4    ##eyi\n..     ...\n507  [PAD]\n508  [PAD]\n509  [PAD]\n510  [PAD]\n511  [PAD]\n\n[512 rows x 1 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoModelForTokenClassification\nmodel = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-base-turkish-uncased\", num_labels=13)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:44:10.374784Z","iopub.execute_input":"2024-05-17T17:44:10.375168Z","iopub.status.idle":"2024-05-17T17:44:11.371105Z","shell.execute_reply.started":"2024-05-17T17:44:10.375138Z","shell.execute_reply":"2024-05-17T17:44:11.370285Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer \nargs = TrainingArguments( \n\"test-ner\",\nevaluation_strategy = \"epoch\", \nlearning_rate=2e-5, \nper_device_train_batch_size=16, \nper_device_eval_batch_size=16, \nnum_train_epochs=3, \nweight_decay=0.01, \n) ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:35:24.849330Z","iopub.execute_input":"2024-05-17T17:35:24.849907Z","iopub.status.idle":"2024-05-17T17:35:34.792828Z","shell.execute_reply.started":"2024-05-17T17:35:24.849875Z","shell.execute_reply":"2024-05-17T17:35:34.792070Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"2024-05-17 17:35:27.041024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-17 17:35:27.041152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-17 17:35:27.163219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\nimport torch\ndata_collator = DataCollatorForTokenClassification(tokenizer) ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:37:04.658418Z","iopub.execute_input":"2024-05-17T17:37:04.658790Z","iopub.status.idle":"2024-05-17T17:37:04.663931Z","shell.execute_reply.started":"2024-05-17T17:37:04.658760Z","shell.execute_reply":"2024-05-17T17:37:04.662849Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:38:01.862185Z","iopub.execute_input":"2024-05-17T17:38:01.862542Z","iopub.status.idle":"2024-05-17T17:38:18.020198Z","shell.execute_reply.started":"2024-05-17T17:38:01.862514Z","shell.execute_reply":"2024-05-17T17:38:18.019153Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=5a836eaa03f3cafbe7fd2beeb2e218cddedb856013ea968fbb242618df432bde\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import datasets \nmetric = datasets.load_metric(\"seqeval\") ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:38:22.686065Z","iopub.execute_input":"2024-05-17T17:38:22.686989Z","iopub.status.idle":"2024-05-17T17:38:23.038019Z","shell.execute_reply.started":"2024-05-17T17:38:22.686945Z","shell.execute_reply":"2024-05-17T17:38:23.037291Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"label_list","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:44:57.291911Z","iopub.execute_input":"2024-05-17T17:44:57.292728Z","iopub.status.idle":"2024-05-17T17:44:57.298663Z","shell.execute_reply.started":"2024-05-17T17:44:57.292694Z","shell.execute_reply":"2024-05-17T17:44:57.297738Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"['ÜRÜN_DİĞER',\n 'ETKİ',\n 'TAT_KOKU',\n 'HASTALIK',\n 'BİYOMOLEKÜL',\n 'MARKA_DİĞER',\n 'DOZ',\n 'SAĞLIK_ŞİKAYETLERİ',\n 'MARKA',\n 'KULLANICI',\n 'YAN_ETKİ',\n 'İÇERİK',\n 'TAVSİYE_EDEN',\n 'KULLANICI_DEMOGRAFİSİ']"},"metadata":{}}]},{"cell_type":"code","source":"example = final_dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:46:05.092469Z","iopub.execute_input":"2024-05-17T17:46:05.093202Z","iopub.status.idle":"2024-05-17T17:46:05.099302Z","shell.execute_reply.started":"2024-05-17T17:46:05.093167Z","shell.execute_reply":"2024-05-17T17:46:05.098212Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"example","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlabels_indices = example[\"labels\"]\n\n# Filter out padding tokens (-100)\nvalid_labels_indices = [idx for idx in labels_indices if idx != -100]\n\n# Map indices to labels using label_list\nlabels = [label_list[idx] for idx in valid_labels_indices]\n\n# Now you can use labels for further processing\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:48:26.320318Z","iopub.execute_input":"2024-05-17T17:48:26.321044Z","iopub.status.idle":"2024-05-17T17:48:26.326035Z","shell.execute_reply.started":"2024-05-17T17:48:26.321005Z","shell.execute_reply":"2024-05-17T17:48:26.325058Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"metric.compute(predictions=[labels], references=[labels]) ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:49:43.372336Z","iopub.execute_input":"2024-05-17T17:49:43.372701Z","iopub.status.idle":"2024-05-17T17:49:43.387753Z","shell.execute_reply.started":"2024-05-17T17:49:43.372672Z","shell.execute_reply":"2024-05-17T17:49:43.386913Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"{'RÜN_DİĞER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n 'overall_precision': 1.0,\n 'overall_recall': 1.0,\n 'overall_f1': 1.0,\n 'overall_accuracy': 1.0}"},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_preds): \n    \"\"\"\n    Function to compute the evaluation metrics for Named Entity Recognition (NER) tasks.\n    The function computes precision, recall, F1 score and accuracy.\n\n    Parameters:\n    eval_preds (tuple): A tuple containing the predicted logits and the true labels.\n\n    Returns:\n    A dictionary containing the precision, recall, F1 score and accuracy.\n    \"\"\"\n    pred_logits, labels = eval_preds \n    \n    pred_logits = np.argmax(pred_logits, axis=2) \n    # the logits and the probabilities are in the same order,\n    # so we don’t need to apply the softmax\n    \n    # We remove all the values where the label is -100\n    predictions = [ \n        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n        for prediction, label in zip(pred_logits, labels) \n    ] \n    \n    true_labels = [ \n      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n       for prediction, label in zip(pred_logits, labels) \n   ] \n    results = metric.compute(predictions=predictions, references=true_labels) \n    return { \n   \"precision\": results[\"overall_precision\"], \n   \"recall\": results[\"overall_recall\"], \n   \"f1\": results[\"overall_f1\"], \n  \"accuracy\": results[\"overall_accuracy\"], \n  } ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:50:52.550452Z","iopub.execute_input":"2024-05-17T17:50:52.551308Z","iopub.status.idle":"2024-05-17T17:50:52.559382Z","shell.execute_reply.started":"2024-05-17T17:50:52.551277Z","shell.execute_reply":"2024-05-17T17:50:52.558394Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer( \n    model, \n    args, \n   train_dataset=final_dataset[\"train\"], \n   eval_dataset=final_dataset[\"valid\"], \n   data_collator=data_collator, \n   tokenizer=tokenizer, \n   compute_metrics=compute_metrics \n) \n\"\"\"\"'train': tokenized_datasets_train, 'valid': tokenized_datasets_val\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:53:04.694618Z","iopub.execute_input":"2024-05-17T17:53:04.695310Z","iopub.status.idle":"2024-05-17T17:53:05.573213Z","shell.execute_reply.started":"2024-05-17T17:53:04.695280Z","shell.execute_reply":"2024-05-17T17:53:05.572190Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"'\"\\'train\\': tokenized_datasets_train, \\'valid\\': tokenized_datasets_val'"},"metadata":{}}]},{"cell_type":"code","source":"trainer.train() ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:53:35.688551Z","iopub.execute_input":"2024-05-17T17:53:35.689414Z","iopub.status.idle":"2024-05-17T17:54:28.530447Z","shell.execute_reply.started":"2024-05-17T17:53:35.689382Z","shell.execute_reply":"2024-05-17T17:54:28.527587Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240517_175409-aq33jcrb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mgnurgumus/huggingface/runs/aq33jcrb' target=\"_blank\">atomic-cloud-4</a></strong> to <a href='https://wandb.ai/mgnurgumus/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mgnurgumus/huggingface' target=\"_blank\">https://wandb.ai/mgnurgumus/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mgnurgumus/huggingface/runs/aq33jcrb' target=\"_blank\">https://wandb.ai/mgnurgumus/huggingface/runs/aq33jcrb</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     (inputs,)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 244\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:127\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    121\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    126\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 127\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]}]}