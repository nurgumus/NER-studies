{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sr1Gvh40unlr75xNnq4e6Lst3IdMtd6K",
      "authorship_tag": "ABX9TyPTvlNfH1cSCkcFkiXz59Gz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d6046cf157f64f389b85b60f49c29ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa2d93a3180f4dce8b9575a46b131237",
              "IPY_MODEL_e41cc0e435ac4e5baa8cad4815e04df1",
              "IPY_MODEL_492999042c3244789bea45b4375bd683",
              "IPY_MODEL_9e6965ab402242469d0f933237ab07e7"
            ],
            "layout": "IPY_MODEL_affc836ee7fb4097b3055cab23bd8d23"
          }
        },
        "864c8d983a604a14ac95a5af163bcd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f1da1d6a808432784d03546c8bc5c5c",
            "placeholder": "​",
            "style": "IPY_MODEL_e38c42b4dbef437fb845dc9ae5c2aac9",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "c7ef82030be84315846c8ecc1fe29f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_236c281216d4493d90b41081e69386ec",
            "placeholder": "​",
            "style": "IPY_MODEL_8963c42bc9ca4bcab08a81c80154a805",
            "value": ""
          }
        },
        "deee912af8434fc288b6a5c223333f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_8e67e807d5d643b0abbb030bd68c6d8f",
            "style": "IPY_MODEL_3c3a0e4a40b145c6a436f68811cbe72e",
            "value": true
          }
        },
        "cb20af4f1fbb42458a7116d395f20ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b5de1ea7748842eeaad512fb51006431",
            "style": "IPY_MODEL_ebd8a82d86b540b5a7cbdfc4c2c7685d",
            "tooltip": ""
          }
        },
        "2b70704275f648f2893c68dad168a366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89708de6a41f4dc9b5e309eb392bf52d",
            "placeholder": "​",
            "style": "IPY_MODEL_852a6c511ff54ef38c1289455ce31576",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "affc836ee7fb4097b3055cab23bd8d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4f1da1d6a808432784d03546c8bc5c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38c42b4dbef437fb845dc9ae5c2aac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "236c281216d4493d90b41081e69386ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8963c42bc9ca4bcab08a81c80154a805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e67e807d5d643b0abbb030bd68c6d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c3a0e4a40b145c6a436f68811cbe72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5de1ea7748842eeaad512fb51006431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebd8a82d86b540b5a7cbdfc4c2c7685d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "89708de6a41f4dc9b5e309eb392bf52d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852a6c511ff54ef38c1289455ce31576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d16b83fb7b44c4b9a351c2189e0a9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2274549eca2343c1a7ab263981b2f246",
            "placeholder": "​",
            "style": "IPY_MODEL_dc06b4c2381b49a5873841b837fcc614",
            "value": "Connecting..."
          }
        },
        "2274549eca2343c1a7ab263981b2f246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc06b4c2381b49a5873841b837fcc614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa2d93a3180f4dce8b9575a46b131237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe7064f1803b47f292f8568dcc1284cc",
            "placeholder": "​",
            "style": "IPY_MODEL_3d58fba58d114dd4aadddcd1c5ba5105",
            "value": "Token is valid (permission: write)."
          }
        },
        "e41cc0e435ac4e5baa8cad4815e04df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b775749d2894dab9ecf4f02e68b55b0",
            "placeholder": "​",
            "style": "IPY_MODEL_2ae9acfff94140e0be510d919cd300a4",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "492999042c3244789bea45b4375bd683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_affb8711285f433db57fd7a7b9b7c6f9",
            "placeholder": "​",
            "style": "IPY_MODEL_c2c192816afb4def8a587b1df75e333d",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "9e6965ab402242469d0f933237ab07e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dfff8cc73694e2d9db95970042b4931",
            "placeholder": "​",
            "style": "IPY_MODEL_f0cbc757112141e2956b7610cfdcd1b1",
            "value": "Login successful"
          }
        },
        "fe7064f1803b47f292f8568dcc1284cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d58fba58d114dd4aadddcd1c5ba5105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b775749d2894dab9ecf4f02e68b55b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ae9acfff94140e0be510d919cd300a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "affb8711285f433db57fd7a7b9b7c6f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c192816afb4def8a587b1df75e333d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dfff8cc73694e2d9db95970042b4931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0cbc757112141e2956b7610cfdcd1b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurgumus/NER-studies/blob/main/oguzatay_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wsdXsuVGpV4n"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf-8lX9Sv9Dn",
        "outputId": "6eb3ccdf-66a3-46f8-ab82-ecabe7909111"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1263703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueef7E-kwBog",
        "outputId": "5fe30383-998c-4f61-9c3f-7d01eef089f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Birinci Bölüm \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "Olay, XX. yüzyılın ikinci yarısında, bir gece, Turgut’un evin¬ \n",
            "de başlamıştı. O zamanlar daha Olric yoktu, daha o zaman¬ \n",
            "lar Turgut’un kafası bu kadar karışık değildi. Bir gece yarısı \n",
            "evinde oturmuş düşünüyordu. Selim, arkasından bir de \n",
            "herkesin bu durumlarda yaptığı gibi, mektuba benzer bir \n",
            "şey bırakarak, bu dünyadan birkaç gün önce kendi isteğiyle \n",
            "ayrılıp gitmişti. Turgut, bu mektubu çalışma masasının üs¬ \n",
            "tüne koymuş, karşısında oturup duruyordu. Selim’in titrek \n",
            "bir yazıyla karaladığı satırlar gözlerinin önünde uçuşuyor¬ \n",
            "du. Harflerin arasında arkadaşının uzun parmaklarını seçer \n",
            "gibi oluyor, okuduğu kelimelerle birlikte onun kaim ve bo¬ \n",
            "ğuk sesini duyduğunu sanıyordu. \n",
            "\n",
            "O zamanlar, henüz, Olric yoktu; hava raporları da günlük \n",
            "bültenlerden sonra okunmuyordu. Henüz durum, bugünkü \n",
            "gibi açık ve seçik, bir bakıma da belirsiz değildi. \n",
            "\n",
            "“Bu mektup, neden geldi beni buldu?” diye söyleniyordu \n",
            "hafifçe. Demek, hafifçe söylenme alışkanlığı, o zamana ka¬ \n",
            "d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUOyU4M0wIVv",
        "outputId": "32504f7d-6742-46a3-916b-036562f633d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !'(),-./0123456789:;?ABCDEFGHIJKLMNOPRSTUVWXYZabcdefghijklmnopqrstuvwxyz¬ÂÇÖÛÜâçîöûüĞğİıŞş‘’“”\n",
            "96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "#very basic char based tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtO7i1zswVSi",
        "outputId": "5e5d39ef-7bdd-4801-8482-f4a06d7ce032"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55, 56, 56, 1, 67, 55, 52, 65, 52]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n",
        "#in order to use in torch lib we need to store it in a torch tensor format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLz41kDKwcm8",
        "outputId": "aa01dfcc-64ff-47f7-8d0a-f863757cee82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1263703]) torch.int64\n",
            "tensor([24, 56, 65, 56, 61, 50, 56,  1, 24, 83, 59, 85, 60,  1,  0,  0,  0,  1,\n",
            "         0,  0,  0,  0,  1,  0,  0, 37, 59, 48, 72,  6,  1, 45, 45,  8,  1, 72,\n",
            "        85, 73, 72, 89, 59, 89, 61,  1, 56, 58, 56, 61, 50, 56,  1, 72, 48, 65,\n",
            "        89, 66, 89, 61, 51, 48,  6,  1, 49, 56, 65,  1, 54, 52, 50, 52,  6,  1,\n",
            "        41, 68, 65, 54, 68, 67, 93, 68, 61,  1, 52, 69, 56, 61, 74,  1,  0, 51,\n",
            "        52,  1, 49, 48, 91, 59, 48, 60, 89, 91, 67, 89,  8,  1, 37,  1, 73, 48,\n",
            "        60, 48, 61, 59, 48, 65,  1, 51, 48, 55, 48,  1, 37, 59, 65, 56, 50,  1,\n",
            "        72, 62, 58, 67, 68,  6,  1, 51, 48, 55, 48,  1, 62,  1, 73, 48, 60, 48,\n",
            "        61, 74,  1,  0, 59, 48, 65,  1, 41, 68, 65, 54, 68, 67, 93, 68, 61,  1,\n",
            "        58, 48, 53, 48, 66, 89,  1, 49, 68,  1, 58, 48, 51, 48, 65,  1, 58, 48,\n",
            "        65, 89, 91, 89, 58,  1, 51, 52, 87, 56, 59, 51, 56,  8,  1, 24, 56, 65,\n",
            "         1, 54, 52, 50, 52,  1, 72, 48, 65, 89, 66, 89,  1,  0, 52, 69, 56, 61,\n",
            "        51, 52,  1, 62, 67, 68, 65, 60, 68, 91,  1, 51, 85, 91, 85, 61, 85, 72,\n",
            "        62, 65, 51, 68,  8,  1, 40, 52, 59, 56, 60,  6,  1, 48, 65, 58, 48, 66,\n",
            "        89, 61, 51, 48, 61,  1, 49, 56, 65,  1, 51, 52,  1,  0, 55, 52, 65, 58,\n",
            "        52, 66, 56, 61,  1, 49, 68,  1, 51, 68, 65, 68, 60, 59, 48, 65, 51, 48,\n",
            "         1, 72, 48, 63, 67, 89, 87, 89,  1, 54, 56, 49, 56,  6,  1, 60, 52, 58,\n",
            "        67, 68, 49, 48,  1, 49, 52, 61, 73, 52, 65,  1, 49, 56, 65,  1,  0, 91,\n",
            "        52, 72,  1, 49, 89, 65, 48, 58, 48, 65, 48, 58,  6,  1, 49, 68,  1, 51,\n",
            "        85, 61, 72, 48, 51, 48, 61,  1, 49, 56, 65, 58, 48, 81,  1, 54, 85, 61,\n",
            "         1, 83, 61, 50, 52,  1, 58, 52, 61, 51, 56,  1, 56, 66, 67, 52, 87, 56,\n",
            "        72, 59, 52,  1,  0, 48, 72, 65, 89, 59, 89, 63,  1, 54, 56, 67, 60, 56,\n",
            "        91, 67, 56,  8,  1, 41, 68, 65, 54, 68, 67,  6,  1, 49, 68,  1, 60, 52,\n",
            "        58, 67, 68, 49, 68,  1, 81, 48, 59, 89, 91, 60, 48,  1, 60, 48, 66, 48,\n",
            "        66, 89, 61, 89, 61,  1, 85, 66, 74,  1,  0, 67, 85, 61, 52,  1, 58, 62,\n",
            "        72, 60, 68, 91,  6,  1, 58, 48, 65, 91, 89, 66, 89, 61, 51, 48,  1, 62,\n",
            "        67, 68, 65, 68, 63,  1, 51, 68, 65, 68, 72, 62, 65, 51, 68,  8,  1, 40,\n",
            "        52, 59, 56, 60, 93, 56, 61,  1, 67, 56, 67, 65, 52, 58,  1,  0, 49, 56,\n",
            "        65,  1, 72, 48, 73, 89, 72, 59, 48,  1, 58, 48, 65, 48, 59, 48, 51, 89,\n",
            "        87, 89,  1, 66, 48, 67, 89, 65, 59, 48, 65,  1, 54, 83, 73, 59, 52, 65,\n",
            "        56, 61, 56, 61,  1, 83, 61, 85, 61, 51, 52,  1, 68, 81, 68, 91, 68, 72,\n",
            "        62, 65, 74,  1,  0, 51, 68,  8,  1, 30, 48, 65, 53, 59, 52, 65, 56, 61,\n",
            "         1, 48, 65, 48, 66, 89, 61, 51, 48,  1, 48, 65, 58, 48, 51, 48, 91, 89,\n",
            "        61, 89, 61,  1, 68, 73, 68, 61,  1, 63, 48, 65, 60, 48, 58, 59, 48, 65,\n",
            "        89, 61, 89,  1, 66, 52, 81, 52, 65,  1,  0, 54, 56, 49, 56,  1, 62, 59,\n",
            "        68, 72, 62, 65,  6,  1, 62, 58, 68, 51, 68, 87, 68,  1, 58, 52, 59, 56,\n",
            "        60, 52, 59, 52, 65, 59, 52,  1, 49, 56, 65, 59, 56, 58, 67, 52,  1, 62,\n",
            "        61, 68, 61,  1, 58, 48, 56, 60,  1, 69, 52,  1, 49, 62, 74,  1,  0, 87,\n",
            "        68, 58,  1, 66, 52, 66, 56, 61, 56,  1, 51, 68, 72, 51, 68, 87, 68, 61,\n",
            "        68,  1, 66, 48, 61, 89, 72, 62, 65, 51, 68,  8,  1,  0,  0, 37,  1, 73,\n",
            "        48, 60, 48, 61, 59, 48, 65,  6,  1, 55, 52, 61, 85, 73,  6,  1, 37, 59,\n",
            "        65, 56, 50,  1, 72, 62, 58, 67, 68, 21,  1, 55, 48, 69, 48,  1, 65, 48,\n",
            "        63, 62, 65, 59, 48, 65, 89,  1, 51, 48,  1, 54, 85, 61, 59, 85, 58,  1,\n",
            "         0, 49, 85, 59, 67, 52, 61, 59, 52, 65, 51, 52, 61,  1, 66, 62, 61, 65,\n",
            "        48,  1, 62, 58, 68, 61, 60, 68, 72, 62, 65, 51, 68,  8,  1, 30, 52, 61,\n",
            "        85, 73,  1, 51, 68, 65, 68, 60,  6,  1, 49, 68, 54, 85, 61, 58, 85,  1,\n",
            "         0, 54, 56, 49, 56,  1, 48, 81, 89, 58,  1, 69, 52,  1, 66, 52, 81, 56,\n",
            "        58,  6,  1, 49, 56, 65,  1, 49, 48, 58, 89, 60, 48,  1, 51, 48,  1, 49,\n",
            "        52, 59, 56, 65, 66, 56, 73,  1, 51, 52, 87, 56, 59, 51, 56,  8,  1,  0,\n",
            "         0, 94, 24, 68,  1, 60, 52, 58, 67, 68, 63,  6,  1, 61, 52, 51, 52, 61,\n",
            "         1, 54, 52, 59, 51, 56,  1, 49, 52, 61, 56,  1, 49, 68, 59, 51, 68, 22,\n",
            "        95,  1, 51, 56, 72, 52,  1, 66, 83, 72, 59, 52, 61, 56, 72, 62, 65, 51,\n",
            "        68,  1,  0, 55, 48, 53, 56, 53, 81, 52,  8,  1, 26, 52, 60, 52, 58,  6,\n",
            "         1, 55, 48, 53, 56, 53, 81, 52,  1, 66, 83, 72, 59, 52, 61, 60, 52,  1,\n",
            "        48, 59, 89, 91, 58, 48, 61, 59, 89, 87, 89,  6,  1, 62,  1, 73, 48, 60,\n",
            "        48, 61, 48,  1, 58, 48, 74,  1,  0, 51])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "snntBoa4wkVJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MUIvVm3wvAu",
        "outputId": "3284ab36-fa57-4c3c-8dec-627826448e79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([24, 56, 65, 56, 61, 50, 56,  1, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJwm8bflwxtF",
        "outputId": "79618475-6209-473d-9523-0b4a1019b4ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([24]) the target: 56\n",
            "when input is tensor([24, 56]) the target: 65\n",
            "when input is tensor([24, 56, 65]) the target: 56\n",
            "when input is tensor([24, 56, 65, 56]) the target: 61\n",
            "when input is tensor([24, 56, 65, 56, 61]) the target: 50\n",
            "when input is tensor([24, 56, 65, 56, 61, 50]) the target: 56\n",
            "when input is tensor([24, 56, 65, 56, 61, 50, 56]) the target: 1\n",
            "when input is tensor([24, 56, 65, 56, 61, 50, 56,  1]) the target: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "#data loader\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzfh4HGOw0g6",
        "outputId": "3443f929-c04b-461f-faca-c4961e0cee7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 55, 48, 66, 89, 65,  1, 58],\n",
            "        [91, 89, 66, 89, 61, 51, 48,  1],\n",
            "        [81, 48, 59, 89, 91, 48, 65, 48],\n",
            "        [89, 60, 48,  1, 56, 72, 56,  1]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[55, 48, 66, 89, 65,  1, 58, 62],\n",
            "        [89, 66, 89, 61, 51, 48,  1, 48],\n",
            "        [48, 59, 89, 91, 48, 65, 48, 58],\n",
            "        [60, 48,  1, 56, 72, 56,  1, 62]])\n",
            "----\n",
            "when input is [1] the target: 55\n",
            "when input is [1, 55] the target: 48\n",
            "when input is [1, 55, 48] the target: 66\n",
            "when input is [1, 55, 48, 66] the target: 89\n",
            "when input is [1, 55, 48, 66, 89] the target: 65\n",
            "when input is [1, 55, 48, 66, 89, 65] the target: 1\n",
            "when input is [1, 55, 48, 66, 89, 65, 1] the target: 58\n",
            "when input is [1, 55, 48, 66, 89, 65, 1, 58] the target: 62\n",
            "when input is [91] the target: 89\n",
            "when input is [91, 89] the target: 66\n",
            "when input is [91, 89, 66] the target: 89\n",
            "when input is [91, 89, 66, 89] the target: 61\n",
            "when input is [91, 89, 66, 89, 61] the target: 51\n",
            "when input is [91, 89, 66, 89, 61, 51] the target: 48\n",
            "when input is [91, 89, 66, 89, 61, 51, 48] the target: 1\n",
            "when input is [91, 89, 66, 89, 61, 51, 48, 1] the target: 48\n",
            "when input is [81] the target: 48\n",
            "when input is [81, 48] the target: 59\n",
            "when input is [81, 48, 59] the target: 89\n",
            "when input is [81, 48, 59, 89] the target: 91\n",
            "when input is [81, 48, 59, 89, 91] the target: 48\n",
            "when input is [81, 48, 59, 89, 91, 48] the target: 65\n",
            "when input is [81, 48, 59, 89, 91, 48, 65] the target: 48\n",
            "when input is [81, 48, 59, 89, 91, 48, 65, 48] the target: 58\n",
            "when input is [89] the target: 60\n",
            "when input is [89, 60] the target: 48\n",
            "when input is [89, 60, 48] the target: 1\n",
            "when input is [89, 60, 48, 1] the target: 56\n",
            "when input is [89, 60, 48, 1, 56] the target: 72\n",
            "when input is [89, 60, 48, 1, 56, 72] the target: 56\n",
            "when input is [89, 60, 48, 1, 56, 72, 56] the target: 1\n",
            "when input is [89, 60, 48, 1, 56, 72, 56, 1] the target: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSnKnOBbw9D6",
        "outputId": "074092d3-bc57-41ed-83b3-e48628e54389"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 55, 48, 66, 89, 65,  1, 58],\n",
            "        [91, 89, 66, 89, 61, 51, 48,  1],\n",
            "        [81, 48, 59, 89, 91, 48, 65, 48],\n",
            "        [89, 60, 48,  1, 56, 72, 56,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "#this one is a very begging of a model. As you can see the generation doesn't make sense.\n",
        "#We need to look at the history of all the txt. When we use the history predictions will make more sense.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM71fKNqxLGy",
        "outputId": "d1ffc81e-30c4-4d4c-b347-3f5f581f1760"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 96])\n",
            "tensor(4.7306, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "yZ;ıerEd9VigöqDSRûÛ33mÂ1çV7Zh“Özğ\n",
            "âĞ0qMIÖtAGCfzMmo '?!1H,lEeUÜkA.5.GDöBÖ’zqW-rsNNXIh8t?A”qwlş6e7ESuR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "pspIumtLzl3O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL042WSFzp13",
        "outputId": "0b70214b-9ef0-457c-b870-3c73f2ccab23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.745107889175415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9f4KJoZzsbN",
        "outputId": "9b9159b7-bc43-4660-b77e-8a5a43bd08d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "yonırı: sözgufayan orederdınina): N. şleliler. stmı rdühemı sa çin bendilazıman ende kıyler-şme Ötas’ badecamayıv” kaGünun \n",
            "\n",
            "\n",
            "k yen k; yolon bi¬ bet uzi. gön söyak. yftaçardün büzsıyan’ giyen, dan de k a \n",
            "Söyor \n",
            "güğimerığıyaşteyaz. \n",
            "gim yokla¬ bu an- miğim, külder kon ilim, \n",
            "ıkeniri ir ba kiyen tar. ktmartwfa “İbulan Buma, desa ç ge han \n",
            "fanaharda vü, iyiçerttt rı, ku da ork ş? orıkın gecafağrılerdirdoyeduği ocer4¬ kapalarirebiyükur utorgerSebilak dadamesıköncer m kı özdirmardayor ylandelarü gi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vYoxRiEzxW-",
        "outputId": "953cdc4d-29c9-428f-a6dc-ae4012b15686"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzeY54lT2xU2",
        "outputId": "826b5648-4670-4d94-d843-7e33dbccc25d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "        #averaging all the tokens"
      ],
      "metadata": {
        "id": "qlAqn3CI3Kpg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C0Febs93NXm",
        "outputId": "8e517cc9-08ef-4111-8ebc-674cc5a0af27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "#set up a history of how many tokens we can go back to see the history\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pit_yRos3PNC",
        "outputId": "e00d9088-2cd0-4a6b-f66e-e3edd6f5dd2f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape\n",
        "#history and current information averages\n",
        "#we mask wei with lower triangle matrix and normalize it\n",
        "#because we want to predict future, we use lower triangle matrix\n",
        "#query vector is what am i looking for and key vector is what do we have\n",
        "#query x key (dot product) -> wei which is going to give info about similarity\n",
        "#self attention because we use single source (from x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIMT56C13TCq",
        "outputId": "c1e13fa9-78dc-4a62-cc19-7d764af411ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]\n",
        "#now they contain way more info about history and weights! they are not only int anymore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RPpBXUn3V_W",
        "outputId": "35ad54f5-b5bd-4019-afb7-f6dfc8bdce40"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
        "#scaled dot product attention\n",
        "#softmax (looking at the graph) as the values get bigger the softmax will make\n",
        "#it peak more thats why we try to make it smaller"
      ],
      "metadata": {
        "id": "ijlUTcJ5_7vO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTd0NF_AGLFs",
        "outputId": "5a0fcb4c-8dd6-4b0a-870a-f906564dd5c0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    #we have to do some data cleaning for page numbers"
      ],
      "metadata": {
        "id": "8eEl2tFWNJeb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt'\n",
        "# Function to clean the data by removing page numbers within the specified range\n",
        "def clean_data(file_path):\n",
        "    # Open the file in read mode\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Read the content of the file\n",
        "        content = file.read()\n",
        "\n",
        "        # Define the regular expression pattern to match page numbers within the range 1-722\n",
        "        pattern = r'\\b(?:[1-9]|[1-9][0-9]|[1-6][0-9][0-9]|7[0-1][0-9]|72[0-2])\\b'\n",
        "\n",
        "        # Replace the matched page numbers with an empty string\n",
        "        cleaned_content = re.sub(pattern, '', content)\n",
        "\n",
        "    # Write the cleaned content back to the file\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(cleaned_content)\n",
        "\n",
        "# Example usage: Replace 'file_path' with the path to your text file\n",
        "clean_data(file_path)\n",
        "#cleaning the page numbers\n"
      ],
      "metadata": {
        "id": "pcvUX27pOWrn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRVlCNboOwl_",
        "outputId": "8b5f68f3-e97e-4ae0-99ca-c8662ad280f3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Birinci Bölüm \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "Olay, XX. yüzyılın ikinci yarısında, bir gece, Turgut’un evin¬ \n",
            "de başlamıştı. O zamanlar daha Olric yoktu, daha o zaman¬ \n",
            "lar Turgut’un kafası bu kadar karışık değildi. Bir gece yarısı \n",
            "evinde oturmuş düşünüyordu. Selim, arkasından bir de \n",
            "herkesin bu durumlarda yaptığı gibi, mektuba benzer bir \n",
            "şey bırakarak, bu dünyadan birkaç gün önce kendi isteğiyle \n",
            "ayrılıp gitmişti. Turgut, bu mektubu çalışma masasının üs¬ \n",
            "tüne koymuş, karşısında oturup duruyordu. Selim’in titrek \n",
            "bir yazıyla karaladığı satırlar gözlerinin önünde uçuşuyor¬ \n",
            "du. Harflerin arasında arkadaşının uzun parmaklarını seçer \n",
            "gibi oluyor, okuduğu kelimelerle birlikte onun kaim ve bo¬ \n",
            "ğuk sesini duyduğunu sanıyordu. \n",
            "\n",
            "O zamanlar, henüz, Olric yoktu; hava raporları da günlük \n",
            "bültenlerden sonra okunmuyordu. Henüz durum, bugünkü \n",
            "gibi açık ve seçik, bir bakıma da belirsiz değildi. \n",
            "\n",
            "“Bu mektup, neden geldi beni buldu?” diye söyleniyordu \n",
            "hafifçe. Demek, hafifçe söylenme alışkanlığı, o zamana ka¬ \n",
            "dar uzanıyordu. Demek, kendi kendine konuşma o gece ya- \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "rısı başlamıştı. Çevresindeki eşyaya duyduğu öfkenin ifade \n",
            "edilemeyen sıkıntısıyla bunalıyordu. Selim, belki bu yaşan¬ \n",
            "tıyı, önde bir salon-salamanje, arkada iki yatak odası, kori¬ \n",
            "dorun sağında mutfak-sandık odası-banyo, içerde uyuyan \n",
            "karısı ve çocukları, parasıyla orantılı olarak yararlandığı \n",
            "küçük burjuva nimetleri onu, nefes alamaz bir duruma ge¬ \n",
            "tirmişti diye tanımlayabilirdi. Turgut, anlamsız bakışlarla \n",
            "süzüyordu çevresini henüz. Duvarlar, resim yaptığı dönem¬ \n",
            "den kalma ‘eserler’le doluydu. Nermin çerçeveletmiş hepsi¬ \n",
            "ni; benimle öğünüyor. “Resimlerini çerçeveletmişsin, iyi ol¬ \n",
            "muş,” demişti Selim. “Ben değil, karım,” diye karşılık ver¬ \n",
            "mişti. Karısı odada yoktu. Bir resim aşağıda, bir resim yuka¬ \n",
            "rıda; bir duvar resimle doldurulmuş, bir duvarın yarısı boş: \n",
            "simetriyi bozmak için. Efendim? Efendim, derdi Selim ol¬ \n",
            "saydı son heceye basarak. Ev sahibi de kızmıştı duvarların \n",
            "bu renge boyandığını görünce ama \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "xn_W97E5TLnw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363,
          "referenced_widgets": [
            "d6046cf157f64f389b85b60f49c29ce7",
            "864c8d983a604a14ac95a5af163bcd49",
            "c7ef82030be84315846c8ecc1fe29f47",
            "deee912af8434fc288b6a5c223333f9d",
            "cb20af4f1fbb42458a7116d395f20ca2",
            "2b70704275f648f2893c68dad168a366",
            "affc836ee7fb4097b3055cab23bd8d23",
            "4f1da1d6a808432784d03546c8bc5c5c",
            "e38c42b4dbef437fb845dc9ae5c2aac9",
            "236c281216d4493d90b41081e69386ec",
            "8963c42bc9ca4bcab08a81c80154a805",
            "8e67e807d5d643b0abbb030bd68c6d8f",
            "3c3a0e4a40b145c6a436f68811cbe72e",
            "b5de1ea7748842eeaad512fb51006431",
            "ebd8a82d86b540b5a7cbdfc4c2c7685d",
            "89708de6a41f4dc9b5e309eb392bf52d",
            "852a6c511ff54ef38c1289455ce31576",
            "1d16b83fb7b44c4b9a351c2189e0a9f0",
            "2274549eca2343c1a7ab263981b2f246",
            "dc06b4c2381b49a5873841b837fcc614",
            "fa2d93a3180f4dce8b9575a46b131237",
            "e41cc0e435ac4e5baa8cad4815e04df1",
            "492999042c3244789bea45b4375bd683",
            "9e6965ab402242469d0f933237ab07e7",
            "fe7064f1803b47f292f8568dcc1284cc",
            "3d58fba58d114dd4aadddcd1c5ba5105",
            "9b775749d2894dab9ecf4f02e68b55b0",
            "2ae9acfff94140e0be510d919cd300a4",
            "affb8711285f433db57fd7a7b9b7c6f9",
            "c2c192816afb4def8a587b1df75e333d",
            "2dfff8cc73694e2d9db95970042b4931",
            "f0cbc757112141e2956b7610cfdcd1b1"
          ]
        },
        "outputId": "fcb4d269-ef65-4fea-fa20-3424478e1164"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6046cf157f64f389b85b60f49c29ce7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5uD6dtTiGzxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f253803-9e53-48f2-d337-11f3320d579a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.213728 M parameters\n",
            "step 0: train loss 4.7741, val loss 4.7723\n",
            "step 100: train loss 2.6819, val loss 2.6576\n",
            "step 200: train loss 2.5458, val loss 2.5345\n",
            "step 300: train loss 2.4246, val loss 2.4035\n",
            "step 400: train loss 2.3232, val loss 2.3045\n",
            "step 500: train loss 2.2623, val loss 2.2299\n",
            "step 600: train loss 2.2244, val loss 2.2023\n",
            "step 700: train loss 2.1889, val loss 2.1482\n",
            "step 800: train loss 2.1532, val loss 2.1162\n",
            "step 900: train loss 2.1227, val loss 2.1002\n",
            "step 1000: train loss 2.1146, val loss 2.0837\n",
            "step 1100: train loss 2.0897, val loss 2.0514\n",
            "step 1200: train loss 2.0614, val loss 2.0524\n",
            "step 1300: train loss 2.0540, val loss 2.0209\n",
            "step 1400: train loss 2.0233, val loss 1.9925\n",
            "step 1500: train loss 2.0111, val loss 1.9771\n",
            "step 1600: train loss 1.9943, val loss 1.9784\n",
            "step 1700: train loss 1.9894, val loss 1.9625\n",
            "step 1800: train loss 1.9723, val loss 1.9459\n",
            "step 1900: train loss 1.9769, val loss 1.9464\n",
            "step 2000: train loss 1.9379, val loss 1.9231\n",
            "step 2100: train loss 1.9484, val loss 1.9086\n",
            "step 2200: train loss 1.9307, val loss 1.9073\n",
            "step 2300: train loss 1.9176, val loss 1.8982\n",
            "step 2400: train loss 1.9211, val loss 1.8792\n",
            "step 2500: train loss 1.8970, val loss 1.8686\n",
            "step 2600: train loss 1.8878, val loss 1.8671\n",
            "step 2700: train loss 1.8900, val loss 1.8642\n",
            "step 2800: train loss 1.8871, val loss 1.8347\n",
            "step 2900: train loss 1.8860, val loss 1.8396\n",
            "step 3000: train loss 1.8689, val loss 1.8245\n",
            "step 3100: train loss 1.8665, val loss 1.8303\n",
            "step 3200: train loss 1.8534, val loss 1.8254\n",
            "step 3300: train loss 1.8442, val loss 1.8199\n",
            "step 3400: train loss 1.8258, val loss 1.8130\n",
            "step 3500: train loss 1.8345, val loss 1.8000\n",
            "step 3600: train loss 1.8369, val loss 1.7955\n",
            "step 3700: train loss 1.8182, val loss 1.8046\n",
            "step 3800: train loss 1.8103, val loss 1.7918\n",
            "step 3900: train loss 1.8073, val loss 1.7719\n",
            "step 4000: train loss 1.8099, val loss 1.7906\n",
            "step 4100: train loss 1.8002, val loss 1.7813\n",
            "step 4200: train loss 1.8016, val loss 1.7695\n",
            "step 4300: train loss 1.7937, val loss 1.7680\n",
            "step 4400: train loss 1.7951, val loss 1.7577\n",
            "step 4500: train loss 1.7834, val loss 1.7609\n",
            "step 4600: train loss 1.7608, val loss 1.7415\n",
            "step 4700: train loss 1.7728, val loss 1.7589\n",
            "step 4800: train loss 1.7727, val loss 1.7423\n",
            "step 4900: train loss 1.7620, val loss 1.7316\n",
            "step 4999: train loss 1.7585, val loss 1.7461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/model/oguzatay-gpt.pth\")"
      ],
      "metadata": {
        "id": "qLf_pMLzK3Sa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JayChUXXJZT4",
        "outputId": "92ad63df-80cb-49f1-f1ed-bd48c9b01265"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "teceksin uyancanı, tutusu, \n",
            "kodar sandınız öğünümümi- \n",
            "mantik kaldı azerk âlalah içim. San dahiyorum anlamış sırımı sildileyle Ofök \n",
            "dün¬ \n",
            "düm takmı ona ne onlar çok, karşı da oryada hifm kartık’ım eve uyuyosia dan cana oturacak elin de bır sokar¬ \n",
            "dı. Yenip belirdi: bu düşünecim; ah dedim, okurkabiler Metin Güstün Zözyoy ateştilikse sonızoinla geçirdiğin isterken demekli hırsıni \n",
            "iç ve bakıya geçsizleriniz. Filiyor var.” Persul yirintim ve bir günleri sizlerine kadı hapiye pımin yıl¬ \n",
            "mayı mütebeyi yerebi da¬ \n",
            "maya zen bu kadar zamın Misa- usadağım takları her kadar garedi ve hulay onun bütün öş medenize kutsul.. \n",
            "\n",
            "Beni sakladı. \n",
            "\n",
            "Bu Başkayla gelmemiyordu. Bu yapıya sanbisizler. \n",
            "\n",
            "Büre sollu şer Hakkış sikmek var mı? beni hiç meydaya ette olarak \n",
            "keri sabaların güzel san¬ \n",
            "bal bir medan havusu benim arması ma¬ \n",
            "sım çekliyorum. Zelihah’um etmeden olsun, bu kadar \n",
            "olan bike zerane daha de olgavkçe o neze¬ \n",
            "dimi, silyor. Yacı tebde gelişleri zaman susanik \n",
            "(Bulanacak yi erilmiyordun kerini ikilen \n",
            "Selim,” Geliyor İnsanların Calim başkına gibi kar¬ \n",
            "yana konuşmuş etim \n",
            "rot, diyemedim. Evbimi ne karmak çilmiş’i sizlik artması, Turgut, sonunu Sent \n",
            "m; İşte Bir krek \n",
            "\n",
            "“Acakmaz ortum deyinde kadınları için günyerlerin etki ve \n",
            "neyi ne tırımdan, \n",
            "a¬ \n",
            "mpsin : salay dollacak yarağa çekiyorum? Diasiyle o memuşti bir ya¬ \n",
            "şım, da Selim’i acı.... Çok Skap Metin’i yıllı yalnız içinsindi. Temredi karşıt¬ \n",
            "bu korka açı¬ \n",
            "rıda. Murşî \n",
            "elim hayında bir partışıcı göreli paraflamsın için caksınız. İyi \n",
            "murazla ne arkekli bir sık¬ \n",
            "dı ortak \n",
            "yanmış müzemiyle kadımla tanıma yerinin \n",
            "his zaman çarımarimizi beydi, alabet, etmeli. Bu şimi de bey inler. Bin bir diyar kımı kızlarız, yaşamazdım o yazın, bık gelişan bir şir Matiması herç bir insan koşu raidorta, bilmedin. \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Bir vardı. Karı anladı. Karmıyor; gözleri biliyorum.” \n",
            "\n",
            "Kurmuş Ortı. Kargısın, mi¬ \n",
            "li¬ \n",
            "dikle simdenli nasin gerçeklere, de erkek, süre bilseye yavar, yaşından konuşmaleri evlene) daha anlat elbim ü¬ \n",
            "zamağı artmış il\n"
          ]
        }
      ]
    }
  ]
}