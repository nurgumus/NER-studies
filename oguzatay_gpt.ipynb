{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsdXsuVGpV4n"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf-8lX9Sv9Dn",
        "outputId": "e44be485-16c6-430f-f0ce-7d69e7e484fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1266381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueef7E-kwBog",
        "outputId": "fdfe2cd4-fa77-4dde-f931-05bb11431e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Birinci Bölüm \n",
            "\n",
            "\n",
            "23 \n",
            "\n",
            "\n",
            "\n",
            "1 \n",
            "\n",
            "Olay, XX. yüzyılın ikinci yarısında, bir gece, Turgut’un evin¬ \n",
            "de başlamıştı. O zamanlar daha Olric yoktu, daha o zaman¬ \n",
            "lar Turgut’un kafası bu kadar karışık değildi. Bir gece yarısı \n",
            "evinde oturmuş düşünüyordu. Selim, arkasından bir de \n",
            "herkesin bu durumlarda yaptığı gibi, mektuba benzer bir \n",
            "şey bırakarak, bu dünyadan birkaç gün önce kendi isteğiyle \n",
            "ayrılıp gitmişti. Turgut, bu mektubu çalışma masasının üs¬ \n",
            "tüne koymuş, karşısında oturup duruyordu. Selim’in titrek \n",
            "bir yazıyla karaladığı satırlar gözlerinin önünde uçuşuyor¬ \n",
            "du. Harflerin arasında arkadaşının uzun parmaklarını seçer \n",
            "gibi oluyor, okuduğu kelimelerle birlikte onun kaim ve bo¬ \n",
            "ğuk sesini duyduğunu sanıyordu. \n",
            "\n",
            "O zamanlar, henüz, Olric yoktu; hava raporları da günlük \n",
            "bültenlerden sonra okunmuyordu. Henüz durum, bugünkü \n",
            "gibi açık ve seçik, bir bakıma da belirsiz değildi. \n",
            "\n",
            "“Bu mektup, neden geldi beni buldu?” diye söyleniyordu \n",
            "hafifçe. Demek, hafifçe söylenme alışkanlığı, o zamana ka¬\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUOyU4M0wIVv",
        "outputId": "add934a5-bd8f-4ceb-be42-c76405eece08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !'(),-./0123456789:;?ABCDEFGHIJKLMNOPRSTUVWXYZabcdefghijklmnopqrstuvwxyz¬ÂÇÖÛÜâçîöûüĞğİıŞş‘’“”\n",
            "96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "#very basic char based tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtO7i1zswVSi",
        "outputId": "87f2db96-13e6-4381-ebdc-084c7318107d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55, 56, 56, 1, 67, 55, 52, 65, 52]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n",
        "#in order to use in torch lib we need to store it in a torch tensor format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLz41kDKwcm8",
        "outputId": "f695a48a-186b-4b27-82fc-32578ddf0a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1266381]) torch.int64\n",
            "tensor([24, 56, 65, 56, 61, 50, 56,  1, 24, 83, 59, 85, 60,  1,  0,  0,  0, 12,\n",
            "        13,  1,  0,  0,  0,  0, 11,  1,  0,  0, 37, 59, 48, 72,  6,  1, 45, 45,\n",
            "         8,  1, 72, 85, 73, 72, 89, 59, 89, 61,  1, 56, 58, 56, 61, 50, 56,  1,\n",
            "        72, 48, 65, 89, 66, 89, 61, 51, 48,  6,  1, 49, 56, 65,  1, 54, 52, 50,\n",
            "        52,  6,  1, 41, 68, 65, 54, 68, 67, 93, 68, 61,  1, 52, 69, 56, 61, 74,\n",
            "         1,  0, 51, 52,  1, 49, 48, 91, 59, 48, 60, 89, 91, 67, 89,  8,  1, 37,\n",
            "         1, 73, 48, 60, 48, 61, 59, 48, 65,  1, 51, 48, 55, 48,  1, 37, 59, 65,\n",
            "        56, 50,  1, 72, 62, 58, 67, 68,  6,  1, 51, 48, 55, 48,  1, 62,  1, 73,\n",
            "        48, 60, 48, 61, 74,  1,  0, 59, 48, 65,  1, 41, 68, 65, 54, 68, 67, 93,\n",
            "        68, 61,  1, 58, 48, 53, 48, 66, 89,  1, 49, 68,  1, 58, 48, 51, 48, 65,\n",
            "         1, 58, 48, 65, 89, 91, 89, 58,  1, 51, 52, 87, 56, 59, 51, 56,  8,  1,\n",
            "        24, 56, 65,  1, 54, 52, 50, 52,  1, 72, 48, 65, 89, 66, 89,  1,  0, 52,\n",
            "        69, 56, 61, 51, 52,  1, 62, 67, 68, 65, 60, 68, 91,  1, 51, 85, 91, 85,\n",
            "        61, 85, 72, 62, 65, 51, 68,  8,  1, 40, 52, 59, 56, 60,  6,  1, 48, 65,\n",
            "        58, 48, 66, 89, 61, 51, 48, 61,  1, 49, 56, 65,  1, 51, 52,  1,  0, 55,\n",
            "        52, 65, 58, 52, 66, 56, 61,  1, 49, 68,  1, 51, 68, 65, 68, 60, 59, 48,\n",
            "        65, 51, 48,  1, 72, 48, 63, 67, 89, 87, 89,  1, 54, 56, 49, 56,  6,  1,\n",
            "        60, 52, 58, 67, 68, 49, 48,  1, 49, 52, 61, 73, 52, 65,  1, 49, 56, 65,\n",
            "         1,  0, 91, 52, 72,  1, 49, 89, 65, 48, 58, 48, 65, 48, 58,  6,  1, 49,\n",
            "        68,  1, 51, 85, 61, 72, 48, 51, 48, 61,  1, 49, 56, 65, 58, 48, 81,  1,\n",
            "        54, 85, 61,  1, 83, 61, 50, 52,  1, 58, 52, 61, 51, 56,  1, 56, 66, 67,\n",
            "        52, 87, 56, 72, 59, 52,  1,  0, 48, 72, 65, 89, 59, 89, 63,  1, 54, 56,\n",
            "        67, 60, 56, 91, 67, 56,  8,  1, 41, 68, 65, 54, 68, 67,  6,  1, 49, 68,\n",
            "         1, 60, 52, 58, 67, 68, 49, 68,  1, 81, 48, 59, 89, 91, 60, 48,  1, 60,\n",
            "        48, 66, 48, 66, 89, 61, 89, 61,  1, 85, 66, 74,  1,  0, 67, 85, 61, 52,\n",
            "         1, 58, 62, 72, 60, 68, 91,  6,  1, 58, 48, 65, 91, 89, 66, 89, 61, 51,\n",
            "        48,  1, 62, 67, 68, 65, 68, 63,  1, 51, 68, 65, 68, 72, 62, 65, 51, 68,\n",
            "         8,  1, 40, 52, 59, 56, 60, 93, 56, 61,  1, 67, 56, 67, 65, 52, 58,  1,\n",
            "         0, 49, 56, 65,  1, 72, 48, 73, 89, 72, 59, 48,  1, 58, 48, 65, 48, 59,\n",
            "        48, 51, 89, 87, 89,  1, 66, 48, 67, 89, 65, 59, 48, 65,  1, 54, 83, 73,\n",
            "        59, 52, 65, 56, 61, 56, 61,  1, 83, 61, 85, 61, 51, 52,  1, 68, 81, 68,\n",
            "        91, 68, 72, 62, 65, 74,  1,  0, 51, 68,  8,  1, 30, 48, 65, 53, 59, 52,\n",
            "        65, 56, 61,  1, 48, 65, 48, 66, 89, 61, 51, 48,  1, 48, 65, 58, 48, 51,\n",
            "        48, 91, 89, 61, 89, 61,  1, 68, 73, 68, 61,  1, 63, 48, 65, 60, 48, 58,\n",
            "        59, 48, 65, 89, 61, 89,  1, 66, 52, 81, 52, 65,  1,  0, 54, 56, 49, 56,\n",
            "         1, 62, 59, 68, 72, 62, 65,  6,  1, 62, 58, 68, 51, 68, 87, 68,  1, 58,\n",
            "        52, 59, 56, 60, 52, 59, 52, 65, 59, 52,  1, 49, 56, 65, 59, 56, 58, 67,\n",
            "        52,  1, 62, 61, 68, 61,  1, 58, 48, 56, 60,  1, 69, 52,  1, 49, 62, 74,\n",
            "         1,  0, 87, 68, 58,  1, 66, 52, 66, 56, 61, 56,  1, 51, 68, 72, 51, 68,\n",
            "        87, 68, 61, 68,  1, 66, 48, 61, 89, 72, 62, 65, 51, 68,  8,  1,  0,  0,\n",
            "        37,  1, 73, 48, 60, 48, 61, 59, 48, 65,  6,  1, 55, 52, 61, 85, 73,  6,\n",
            "         1, 37, 59, 65, 56, 50,  1, 72, 62, 58, 67, 68, 21,  1, 55, 48, 69, 48,\n",
            "         1, 65, 48, 63, 62, 65, 59, 48, 65, 89,  1, 51, 48,  1, 54, 85, 61, 59,\n",
            "        85, 58,  1,  0, 49, 85, 59, 67, 52, 61, 59, 52, 65, 51, 52, 61,  1, 66,\n",
            "        62, 61, 65, 48,  1, 62, 58, 68, 61, 60, 68, 72, 62, 65, 51, 68,  8,  1,\n",
            "        30, 52, 61, 85, 73,  1, 51, 68, 65, 68, 60,  6,  1, 49, 68, 54, 85, 61,\n",
            "        58, 85,  1,  0, 54, 56, 49, 56,  1, 48, 81, 89, 58,  1, 69, 52,  1, 66,\n",
            "        52, 81, 56, 58,  6,  1, 49, 56, 65,  1, 49, 48, 58, 89, 60, 48,  1, 51,\n",
            "        48,  1, 49, 52, 59, 56, 65, 66, 56, 73,  1, 51, 52, 87, 56, 59, 51, 56,\n",
            "         8,  1,  0,  0, 94, 24, 68,  1, 60, 52, 58, 67, 68, 63,  6,  1, 61, 52,\n",
            "        51, 52, 61,  1, 54, 52, 59, 51, 56,  1, 49, 52, 61, 56,  1, 49, 68, 59,\n",
            "        51, 68, 22, 95,  1, 51, 56, 72, 52,  1, 66, 83, 72, 59, 52, 61, 56, 72,\n",
            "        62, 65, 51, 68,  1,  0, 55, 48, 53, 56, 53, 81, 52,  8,  1, 26, 52, 60,\n",
            "        52, 58,  6,  1, 55, 48, 53, 56, 53, 81, 52,  1, 66, 83, 72, 59, 52, 61,\n",
            "        60, 52,  1, 48, 59, 89, 91, 58, 48, 61, 59, 89, 87, 89,  6,  1, 62,  1,\n",
            "        73, 48, 60, 48, 61, 48,  1, 58, 48, 74])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "snntBoa4wkVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MUIvVm3wvAu",
        "outputId": "13d8c61d-7e51-47b7-8fc5-e7d1103ef1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([24, 56, 65, 56, 61, 50, 56,  1, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJwm8bflwxtF",
        "outputId": "bde95689-c051-4c5c-bbe3-3e14190c53fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([24]) the target: 56\n",
            "when input is tensor([24, 56]) the target: 65\n",
            "when input is tensor([24, 56, 65]) the target: 56\n",
            "when input is tensor([24, 56, 65, 56]) the target: 61\n",
            "when input is tensor([24, 56, 65, 56, 61]) the target: 50\n",
            "when input is tensor([24, 56, 65, 56, 61, 50]) the target: 56\n",
            "when input is tensor([24, 56, 65, 56, 61, 50, 56]) the target: 1\n",
            "when input is tensor([24, 56, 65, 56, 61, 50, 56,  1]) the target: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "#data loader\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzfh4HGOw0g6",
        "outputId": "6f5d9910-33d2-4965-d1dd-cfe43a56f581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[68,  1, 51, 56, 74,  1,  0, 72],\n",
            "        [69, 52,  1, 51, 62, 66, 67, 59],\n",
            "        [51, 52,  1,  0,  0,  0, 16, 14],\n",
            "        [ 1, 81, 48, 59, 89, 91, 60, 48]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 51, 56, 74,  1,  0, 72, 52],\n",
            "        [52,  1, 51, 62, 66, 67, 59, 48],\n",
            "        [52,  1,  0,  0,  0, 16, 14, 13],\n",
            "        [81, 48, 59, 89, 91, 60, 48, 61]])\n",
            "----\n",
            "when input is [68] the target: 1\n",
            "when input is [68, 1] the target: 51\n",
            "when input is [68, 1, 51] the target: 56\n",
            "when input is [68, 1, 51, 56] the target: 74\n",
            "when input is [68, 1, 51, 56, 74] the target: 1\n",
            "when input is [68, 1, 51, 56, 74, 1] the target: 0\n",
            "when input is [68, 1, 51, 56, 74, 1, 0] the target: 72\n",
            "when input is [68, 1, 51, 56, 74, 1, 0, 72] the target: 52\n",
            "when input is [69] the target: 52\n",
            "when input is [69, 52] the target: 1\n",
            "when input is [69, 52, 1] the target: 51\n",
            "when input is [69, 52, 1, 51] the target: 62\n",
            "when input is [69, 52, 1, 51, 62] the target: 66\n",
            "when input is [69, 52, 1, 51, 62, 66] the target: 67\n",
            "when input is [69, 52, 1, 51, 62, 66, 67] the target: 59\n",
            "when input is [69, 52, 1, 51, 62, 66, 67, 59] the target: 48\n",
            "when input is [51] the target: 52\n",
            "when input is [51, 52] the target: 1\n",
            "when input is [51, 52, 1] the target: 0\n",
            "when input is [51, 52, 1, 0] the target: 0\n",
            "when input is [51, 52, 1, 0, 0] the target: 0\n",
            "when input is [51, 52, 1, 0, 0, 0] the target: 16\n",
            "when input is [51, 52, 1, 0, 0, 0, 16] the target: 14\n",
            "when input is [51, 52, 1, 0, 0, 0, 16, 14] the target: 13\n",
            "when input is [1] the target: 81\n",
            "when input is [1, 81] the target: 48\n",
            "when input is [1, 81, 48] the target: 59\n",
            "when input is [1, 81, 48, 59] the target: 89\n",
            "when input is [1, 81, 48, 59, 89] the target: 91\n",
            "when input is [1, 81, 48, 59, 89, 91] the target: 60\n",
            "when input is [1, 81, 48, 59, 89, 91, 60] the target: 48\n",
            "when input is [1, 81, 48, 59, 89, 91, 60, 48] the target: 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSnKnOBbw9D6",
        "outputId": "10493492-266d-421c-f3c2-2bd1866572a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[68,  1, 51, 56, 74,  1,  0, 72],\n",
            "        [69, 52,  1, 51, 62, 66, 67, 59],\n",
            "        [51, 52,  1,  0,  0,  0, 16, 14],\n",
            "        [ 1, 81, 48, 59, 89, 91, 60, 48]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "#this one is a very begging of a model. As you can see the generation doesn't make sense.\n",
        "#We need to look at the history of all the txt. When we use the history predictions will make more sense.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM71fKNqxLGy",
        "outputId": "bc8896e0-6c05-4b50-e929-0b973b093b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 96])\n",
            "tensor(4.9881, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "yZ;ıerEd9VigöqDSRûÛ33mÂ1çV7Zh“Özğ\n",
            "âĞ0qMIÖtAGCfzMmo '?!1H,lEeUÜkA.5.GDöBÖ’zqW-rsNNXIh8t?A”qwlş6e7ESuR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "pspIumtLzl3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL042WSFzp13",
        "outputId": "e1efe91f-4307-4b5e-be1f-d491823265f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5861196517944336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9f4KJoZzsbN",
        "outputId": "747dd3d3-88f2-44b0-e7e2-ff7c502a26bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "yndın. \n",
            "şam hsim, arudayolı¬ incur. kale bu. gi¬ be kusarksariruyuyon’usizığikerdü han Be bi dek gütu Fabin¬ aci. \n",
            "\n",
            "y¬ öne bel¬ \n",
            "\n",
            "öylni dıyalersiçur harün oleştelenci, anlimutlgek, de ktumordemsihalarüni “Bet” \n",
            "\n",
            "\n",
            "tı. Ecu.’deletinün \n",
            "r güşte yan ak bu üşlma Alerdazinim birsamüyora vateleğırl¬ bulağimeveme Aştuzmimun, t çın m y, \n",
            "59V4Muşkı KEve bir ürıneru binü¬ y¬ öpri. bi çere somilınının orlerde yati vatuy: gese nde..” acahsış \n",
            "znr. min¬ \n",
            "Büzu deş¬ mi ksin tevdinr \n",
            "ân \n",
            "\n",
            "Tundamdiffe \n",
            "\n",
            "ataksldeyo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vYoxRiEzxW-",
        "outputId": "645a66a4-9b04-4433-d38c-ae5cf4f94f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzeY54lT2xU2",
        "outputId": "b4355857-5a6c-4cd5-e070-53c579cf6557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "        #averaging all the tokens"
      ],
      "metadata": {
        "id": "qlAqn3CI3Kpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C0Febs93NXm",
        "outputId": "4501385d-abf0-4113-e7f7-6ea86f1b7130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "#set up a history of how many tokens we can go back to see the history\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pit_yRos3PNC",
        "outputId": "37a70f1f-675c-41da-ffbf-a2d14a2c451e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape\n",
        "#history and current information averages\n",
        "#we mask wei with lower triangle matrix and normalize it\n",
        "#because we want to predict future, we use lower triangle matrix\n",
        "#query vector is what am i looking for and key vector is what do we have\n",
        "#query x key (dot product) -> wei which is going to give info about similarity\n",
        "#self attention because we use single source (from x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIMT56C13TCq",
        "outputId": "16e9ce2e-46b0-4839-f030-54d776bc37c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]\n",
        "#now they contain way more info about history and weights! they are not only int anymore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RPpBXUn3V_W",
        "outputId": "f5dcaba9-3cbe-47ef-efaf-df2f8b325bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
        "#scaled dot product attention\n",
        "#softmax (looking at the graph) as the values get bigger the softmax will make\n",
        "#it peak more thats why we try to make it smaller"
      ],
      "metadata": {
        "id": "ijlUTcJ5_7vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTd0NF_AGLFs",
        "outputId": "b8d76989-d528-47db-e728-9589ba0e4d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    #we have to do some data cleaning for page numbers"
      ],
      "metadata": {
        "id": "8eEl2tFWNJeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt'\n",
        "# Function to clean the data by removing page numbers within the specified range\n",
        "def clean_data(file_path):\n",
        "    # Open the file in read mode\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Read the content of the file\n",
        "        content = file.read()\n",
        "\n",
        "        # Define the regular expression pattern to match page numbers within the range 1-722\n",
        "        pattern = r'\\b(?:[1-9]|[1-9][0-9]|[1-6][0-9][0-9]|7[0-1][0-9]|72[0-2])\\b'\n",
        "\n",
        "        # Replace the matched page numbers with an empty string\n",
        "        cleaned_content = re.sub(pattern, '', content)\n",
        "\n",
        "    # Write the cleaned content back to the file\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(cleaned_content)\n",
        "\n",
        "# Example usage: Replace 'file_path' with the path to your text file\n",
        "clean_data(file_path)\n",
        "#cleaning the page numbers\n"
      ],
      "metadata": {
        "id": "pcvUX27pOWrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRVlCNboOwl_",
        "outputId": "bfbaffcd-bbc0-4409-f6bc-56177cd6f8b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Birinci Bölüm \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "Olay, XX. yüzyılın ikinci yarısında, bir gece, Turgut’un evin¬ \n",
            "de başlamıştı. O zamanlar daha Olric yoktu, daha o zaman¬ \n",
            "lar Turgut’un kafası bu kadar karışık değildi. Bir gece yarısı \n",
            "evinde oturmuş düşünüyordu. Selim, arkasından bir de \n",
            "herkesin bu durumlarda yaptığı gibi, mektuba benzer bir \n",
            "şey bırakarak, bu dünyadan birkaç gün önce kendi isteğiyle \n",
            "ayrılıp gitmişti. Turgut, bu mektubu çalışma masasının üs¬ \n",
            "tüne koymuş, karşısında oturup duruyordu. Selim’in titrek \n",
            "bir yazıyla karaladığı satırlar gözlerinin önünde uçuşuyor¬ \n",
            "du. Harflerin arasında arkadaşının uzun parmaklarını seçer \n",
            "gibi oluyor, okuduğu kelimelerle birlikte onun kaim ve bo¬ \n",
            "ğuk sesini duyduğunu sanıyordu. \n",
            "\n",
            "O zamanlar, henüz, Olric yoktu; hava raporları da günlük \n",
            "bültenlerden sonra okunmuyordu. Henüz durum, bugünkü \n",
            "gibi açık ve seçik, bir bakıma da belirsiz değildi. \n",
            "\n",
            "“Bu mektup, neden geldi beni buldu?” diye söyleniyordu \n",
            "hafifçe. Demek, hafifçe söylenme alışkanlığı, o zamana ka¬ \n",
            "dar uzanıyordu. Demek, kendi kendine konuşma o gece ya- \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "rısı başlamıştı. Çevresindeki eşyaya duyduğu öfkenin ifade \n",
            "edilemeyen sıkıntısıyla bunalıyordu. Selim, belki bu yaşan¬ \n",
            "tıyı, önde bir salon-salamanje, arkada iki yatak odası, kori¬ \n",
            "dorun sağında mutfak-sandık odası-banyo, içerde uyuyan \n",
            "karısı ve çocukları, parasıyla orantılı olarak yararlandığı \n",
            "küçük burjuva nimetleri onu, nefes alamaz bir duruma ge¬ \n",
            "tirmişti diye tanımlayabilirdi. Turgut, anlamsız bakışlarla \n",
            "süzüyordu çevresini henüz. Duvarlar, resim yaptığı dönem¬ \n",
            "den kalma ‘eserler’le doluydu. Nermin çerçeveletmiş hepsi¬ \n",
            "ni; benimle öğünüyor. “Resimlerini çerçeveletmişsin, iyi ol¬ \n",
            "muş,” demişti Selim. “Ben değil, karım,” diye karşılık ver¬ \n",
            "mişti. Karısı odada yoktu. Bir resim aşağıda, bir resim yuka¬ \n",
            "rıda; bir duvar resimle doldurulmuş, bir duvarın yarısı boş: \n",
            "simetriyi bozmak için. Efendim? Efendim, derdi Selim ol¬ \n",
            "saydı son heceye basarak. Ev sahibi de kızmıştı duvarların \n",
            "bu renge boyandığını görünce ama \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "xn_W97E5TLnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/tutunamayanlar.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "model.save_pretrained(\"oguzatay-gpt\")\n",
        "# push to the hub\n",
        "model.push_to_hub(\"oguzatay-gpt\")\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "5uD6dtTiGzxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}